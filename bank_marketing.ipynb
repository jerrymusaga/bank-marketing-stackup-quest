{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6b542f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a478ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import common libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import specific components from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, RocCurveDisplay\n",
    "\n",
    "# enhanced stats functions\n",
    "from scipy import stats\n",
    "\n",
    "# for ease of data profiling\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc8edbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version  : 1.4.3\n",
      "numpy version   : 1.23.2\n",
      "seaborn version : 0.11.2\n"
     ]
    }
   ],
   "source": [
    "# print environment setup details\n",
    "print(f\"pandas version  : {pd.__version__}\")  # 1.4.3\n",
    "print(f\"numpy version   : {np.__version__}\")  # 1.23.1\n",
    "print(f\"seaborn version : {sns.__version__}\") # 0.11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ee0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of columns for display so we can see all of them\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31621c8",
   "metadata": {},
   "source": [
    "Here, we also set the seed for numpy's random number generator such that our results are fully reproducible. This is because the other libraries (e.g. scikit-learn) use this random number generator, so if we set the seed we will always generate the same random numbers in the same sequence.\n",
    "\n",
    "Thus, whenever we run the notebook from top-to-bottom, we will end up with the *exact* same results! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c8144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350d6ac",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "To load in the data for this project, read in `bank-additional-full.csv` into a variable called `df` as a pandas DataFrame. The first two rows of the DataFrame should look like this:\n",
    "\n",
    "|     | age | job | marital | education | default | housing | loan | contact | month | day_of_week | duration | campaign | pdays | previous | poutcome | emp.var.rate | cons.price.idx | cons.conf.idx | euribor3m | nr.employed | y |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| **0** | 56 | housemaid | married | basic.4y | no | no | no | telephone | may | mon | 261 | 1 | 999 | 0 | nonexistent | 1.1 | 93.994 | -36.4 | 4.857 | 5191.0 | no |\n",
    "| **1** | 57 | services | married | high.school | unknown | no | no | telephone | may | mon | 149 | 1 | 999 | 0 | nonexistent | 1.1 | 93.994 | -36.4 | 4.857 | 5191.0 | no |\n",
    "\n",
    "Note that the file may not be using a conventional delimiter (e.g. comma, tabs, etc.). Figure out what the delimiter is and read the file accordingly :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cef3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df = pd.read_csv('bank-additional-full.csv',sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7b56d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "\n",
       "  month day_of_week  duration  campaign  pdays  previous     poutcome  \\\n",
       "0   may         mon       261         1    999         0  nonexistent   \n",
       "1   may         mon       149         1    999         0  nonexistent   \n",
       "\n",
       "   emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0           1.1          93.994          -36.4      4.857       5191.0  no  \n",
       "1           1.1          93.994          -36.4      4.857       5191.0  no  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure data was read in properly and matches above table\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036976a",
   "metadata": {},
   "source": [
    "# EDA\n",
    "Now it is time for you to practice your EDA skills! Take a look at the Quest 3 Quiz, and answer the first 4 questions on the results of your EDA on the dataset.\n",
    "\n",
    "This time, you don't get the data dictionary! You have to go look at the dataset's website and take a look yourself, like you would have to if this was a real project.\n",
    "\n",
    "Here's the website where the dataset came from: [the UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/bank+marketing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db665e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.11360645294189453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Summarize dataset",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89519187a1e44528e22abd6eb1f00a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.4022383689880371,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generate report structure",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94502b26ca4a4120932bec5ee83ce67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.27313733100891113,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Render HTML",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e78e865ea8a4f44aa3bf4f0ed641214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.5874419212341309,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Export report to file",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed987e8c7b024e1f8cb3cfe3c61be135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# optional: use pandas-profiling to do quick first pass at EDA\n",
    "profile = ProfileReport(df, title=\"Bank Marketing Dataset Profiling Report\")\n",
    "# create html file to view report\n",
    "profile.to_file(\"bank_marketing_dataset_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73741f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>41188.00000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "      <td>41188.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.02406</td>\n",
       "      <td>258.285010</td>\n",
       "      <td>2.567593</td>\n",
       "      <td>962.475454</td>\n",
       "      <td>0.172963</td>\n",
       "      <td>0.081886</td>\n",
       "      <td>93.575664</td>\n",
       "      <td>-40.502600</td>\n",
       "      <td>3.621291</td>\n",
       "      <td>5167.035911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.42125</td>\n",
       "      <td>259.279249</td>\n",
       "      <td>2.770014</td>\n",
       "      <td>186.910907</td>\n",
       "      <td>0.494901</td>\n",
       "      <td>1.570960</td>\n",
       "      <td>0.578840</td>\n",
       "      <td>4.628198</td>\n",
       "      <td>1.734447</td>\n",
       "      <td>72.251528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.400000</td>\n",
       "      <td>92.201000</td>\n",
       "      <td>-50.800000</td>\n",
       "      <td>0.634000</td>\n",
       "      <td>4963.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.00000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.800000</td>\n",
       "      <td>93.075000</td>\n",
       "      <td>-42.700000</td>\n",
       "      <td>1.344000</td>\n",
       "      <td>5099.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.00000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>93.749000</td>\n",
       "      <td>-41.800000</td>\n",
       "      <td>4.857000</td>\n",
       "      <td>5191.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>47.00000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>93.994000</td>\n",
       "      <td>-36.400000</td>\n",
       "      <td>4.961000</td>\n",
       "      <td>5228.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>98.00000</td>\n",
       "      <td>4918.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>94.767000</td>\n",
       "      <td>-26.900000</td>\n",
       "      <td>5.045000</td>\n",
       "      <td>5228.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age      duration      campaign         pdays      previous  \\\n",
       "count  41188.00000  41188.000000  41188.000000  41188.000000  41188.000000   \n",
       "mean      40.02406    258.285010      2.567593    962.475454      0.172963   \n",
       "std       10.42125    259.279249      2.770014    186.910907      0.494901   \n",
       "min       17.00000      0.000000      1.000000      0.000000      0.000000   \n",
       "25%       32.00000    102.000000      1.000000    999.000000      0.000000   \n",
       "50%       38.00000    180.000000      2.000000    999.000000      0.000000   \n",
       "75%       47.00000    319.000000      3.000000    999.000000      0.000000   \n",
       "max       98.00000   4918.000000     56.000000    999.000000      7.000000   \n",
       "\n",
       "       emp.var.rate  cons.price.idx  cons.conf.idx     euribor3m   nr.employed  \n",
       "count  41188.000000    41188.000000   41188.000000  41188.000000  41188.000000  \n",
       "mean       0.081886       93.575664     -40.502600      3.621291   5167.035911  \n",
       "std        1.570960        0.578840       4.628198      1.734447     72.251528  \n",
       "min       -3.400000       92.201000     -50.800000      0.634000   4963.600000  \n",
       "25%       -1.800000       93.075000     -42.700000      1.344000   5099.100000  \n",
       "50%        1.100000       93.749000     -41.800000      4.857000   5191.000000  \n",
       "75%        1.400000       93.994000     -36.400000      4.961000   5228.100000  \n",
       "max        1.400000       94.767000     -26.900000      5.045000   5228.100000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conduct any other EDA that you need to in order to get a good feel for the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9aaa8",
   "metadata": {},
   "source": [
    "With the EDA you have conducted, answer the following questions from the quiz. Note that these questions do not cover everything you should be looking for when doing EDA, they are just to give you an idea of what EDA would look like on such a dataset.\n",
    "## Question 1\n",
    "What percentage of users ended up subscribing to a deposit? (i.e. column `y` == 'yes') Round your answer to 1 decimal place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage of users where y == \"yes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29125623",
   "metadata": {},
   "source": [
    "This is interesting, it seems the vast majority of users did not subscribe to a deposit. This is very important to note - we have what's called an \"imbalanced classification\" problem - i.e. we do not have balance in the classes that we are classifying. This affects how we will handle our modelling later, and we'll revisit this. For now, just keep this fact in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b7ca6",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "How many duplicate rows are in the dataset? (i.e. fewest number of rows you would have to remove in order to be left without duplicate rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ffdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine number of duplicate rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf2b98",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "What does a `pdays` value of 999 mean?\n",
    "\n",
    "(hint: you'll have to look at the dataset website!)\n",
    "\n",
    "## Question 4\n",
    "Are there any high correlations (e.g. Â±0.5) between independent variables? If yes, how high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef19dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation heatmap to get a sense of what variables are correlated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3de18",
   "metadata": {},
   "source": [
    "Wow, that's problematic. There will very likely be multicollinearity issues if we don't deal with that when wrangling our data. Let's deal with the data wrangling now.\n",
    "\n",
    "# Data Wrangling\n",
    "As we did before in Quest 2, once we've adequately explored our data, we need to wrangle it for modelling.\n",
    "\n",
    "In this case, we also don't have any missing data, but we do have several duplicate rows, a lot of independent variables correlated with one another, and a LOT of categorical variables! \n",
    "\n",
    "One step at a time. First, let's deal with the duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f482a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates in-place, and reset the index so no index values are skipped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to make sure nothing left is duplicated\n",
    "assert df.duplicated().any() == False, \"Some duplicates remain.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50534080",
   "metadata": {},
   "source": [
    "Now, let's drop some variables. If you read closely, the dataset instructs us to discard the `duration` variable \"if the intention is to have a realistic predictive model\", which ours is.\n",
    "\n",
    "If you don't know where that came from, go back and check the dataset webpage again. This is an important practice to note!\n",
    "\n",
    "Drop the following variables from df:\n",
    "* duration\n",
    "* emp.var.rate\n",
    "* cons.price.idx\n",
    "* cons.conf.idx\n",
    "* euribor3m\n",
    "* nr.employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce25e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop variables from df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"variables after dropping: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd7a90",
   "metadata": {},
   "source": [
    "Now, get dummy variables for all our categorical variables using `pd.get_dummies()`, ensuring to use the `drop_first=True` argument to mitigate possible multicollinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies for all categorical variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af4089",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "How many variables were left after dropping variables, and after getting dummies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b70ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"variables after getting dummies: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea93543",
   "metadata": {},
   "source": [
    "Now verify that we still don't have major multicollinearity issues. We have a few too many variables to use the heatmap directly, so you'll have to be clever and try to figure out another way of doing it.\n",
    "\n",
    "Here's a hint - previously, we called `df.corr()` which gave us the pair-wise correlations of each variable, and then plotted it using the heatmap. Maybe we can start with `df.corr()` and do something else to find highly correlated variables? Something that may help is the `DataFrame.itertuples()` method - [documentation here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html).\n",
    "\n",
    "(If you can't figure it out, I put one possible solution all the way at the bottom of the notebook. But for the purposes of learning, I suggest you try and struggle at this problem for at least a little while first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270914f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get list of all variable combinations from highest correlation to lowest\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619cffbe",
   "metadata": {},
   "source": [
    "Regardless, there are still a number of variables which are still strongly correlated to each other. In fact, there are some which are *exactly* identical to one another! So we'll remove a few more variables:\n",
    "* loan_unknown\n",
    "* poutcome_success\n",
    "* poutcome_nonexistent\n",
    "\n",
    "In theory, a better approach would likely be to remove variables depending on whether our model improves or not when they are removed. For the sake of simplicity, we're doing this analysis just from data to model, but normally your model's performance would feedback on the choices of your data handling/variable selection/feature engineering, and you'd adjust accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0409df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop these 3 other variables from df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6996c8a5",
   "metadata": {},
   "source": [
    "While we're at it, the column named `y_yes` is a bit silly. Let's rename it to something more informative, like `deposit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename \"y_yes\" column to \"deposit\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16fadca",
   "metadata": {},
   "source": [
    "Excellent! Now separate our independent variables into a variable called `X`, and our target variable `deposit` into a variable called `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up our variables\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the resulting shapes of X and y should be (41176, 44) and (41176,) respectively\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27a566",
   "metadata": {},
   "source": [
    "Now we need to split up our data into training and test data. Using scikit-learn's `train_test_split` function, using a `test_size` of 0.3 (i.e. 30% of data in test set), and ensure that the random state is set to our seed from above.\n",
    "\n",
    "Documentation for `train_test_split()` can be found here: https://scikit-learn.org/1.1/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bbb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the train test split\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f120b",
   "metadata": {},
   "source": [
    "Now let's verify to make sure you split your data identically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31311d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"expected : 29491, 26771, 22437, 6760, 28406\")\n",
    "print(\"actual   : \" + \", \".join([str(x) for x in X_train.head().index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1a4da",
   "metadata": {},
   "source": [
    "Now, we're going to do something that we didn't do in Quest 2 (because I didn't want to introduce too many concepts at once), but is a machine learning best practice - standardizing your dataset!\n",
    "\n",
    "Essentially, if some numeric variables are much bigger than the others (e.g. age goes up to the 90s, whereas our one-hot encoded dummy variables are only 0 or 1), they may have a larger impact on the final model. As such, we scale all our variables so comparisons between them can be made fairly.\n",
    "\n",
    "Scikit-learn provides a helpful utility for this, `StandardScaler()` - its [documentation is here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). \n",
    "\n",
    "### Optional\n",
    "If you go back and scale your features in Quest 2, does the model perform better with or without scaling? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit scaler\n",
    "scaler = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_train and X_test\n",
    "X_train = \n",
    "X_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6471b31",
   "metadata": {},
   "source": [
    "If you see an array which is a lot of numbers generally ranging from -2 to +2, then your scaling should have worked! Nicely done.\n",
    "\n",
    "# Modelling and Evaluation\n",
    "Now, you might remember that our dataset is an imbalanced dataset, with far more values of `0` for deposit than `1`. If we don't properly handle this, we're going to have trouble! For example, your model could end up with 90% accuracy if it just said that every single value was `0`, which is not something that we want.\n",
    "\n",
    "Just to name a few ways of solving this problem (this is not an exhaustive list!):\n",
    "1. Sub-sample the `0`s, throwing away most of the data until we have an even split of `0` and `1`.\n",
    "2. Over-sample the `1`s, duplicating much of the `1` data until we have an even split.\n",
    "3. Weight the model towards the minority class (`1` in this case) so that it places extra emphasis on it while learning.\n",
    "\n",
    "Each of these approaches has different pros and cons. For this quest, we'll be doing approach 3, which scikit-learn has a convenient parameter for. \n",
    "\n",
    "So now let's make our model! Check the docs for the `LogisticRegression()` scikit-learn function to figure out what the `class_weight` parameter does, then make and fit the training data to your model.\n",
    "\n",
    "Documentation for `LogisticRegression()` can be found here: https://scikit-learn.org/1.1/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd8bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate logistic regression model and fit the training data to it\n",
    "lr = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aebe4f",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "For your trained logistic regression model, what is the intercept and the coefficient for the `age` feature?\n",
    "\n",
    "Hint: check the LogisticRegression() documentation to see how to find these attributes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5867ea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# describe model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b3c51",
   "metadata": {},
   "source": [
    "Now we need to evaluate how our model did. Let's get the score, classification report, confusion matrix, and ROC curve, just as was done in the lesson video.\n",
    "\n",
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f3b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict training set values\n",
    "predictions_train = \n",
    "# classification report on training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix for training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72bf13",
   "metadata": {},
   "source": [
    "Note that the `plot_roc_curve()` function used in the video is actually deprecated, and it is recommended to use either `RocCurveDisplay.from_estimator()` or `RocCurveDisplay.from_predictions()` now.\n",
    "\n",
    "See the documentation for further details: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef059bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use RocCurveDisplay to plot a ROC curve on training set\n",
    "RocCurveDisplay.from_estimator(lr, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f91df2",
   "metadata": {},
   "source": [
    "Wow, those results aren't impressive at all. Our ability to make positive predictions is somewhat underwhelming. But, of course, perhaps that truly is the best we can do with the data we used. Let's move on and evaluate the same metrics on the test set first, before we draw any rash conclusions.\n",
    "\n",
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc870a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test set values\n",
    "\n",
    "# classification report on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19bcd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix for test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c128f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use RocCurveDisplay to plot a ROC curve on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be17be4",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "While our model has clearly learned something about the data, since it's performing better than chance, there is still a lot of room for improvement here.\n",
    "\n",
    "But before you get too demoralised - this is a much more realistic example of what it looks like when you model real data! Real data is messy, with a lot of complex interactions that are hard to anticipate when first beginning a modelling exercise. As a result, it's rare that your first attempt to model a dataset works out perfectly. Modelling is often an iterative process, with the results from each round informing subsequent modelling attempts. \n",
    "\n",
    "Also, data on human behaviour is often messy - it's hard to precisely predict humans, given how complex our motives and justifications for our actions often are.\n",
    "\n",
    "So, honestly, this is a pretty realistic first attempt at modelling! What could we try in subsequent models? Here's a non-exhaustive list of ideas:\n",
    "1. Our feature selection process was very ad hoc - perhaps we should try selecting features more deliberately. We might have left out some valuable information when throwing out variables, or we may have left in redundant/unnecessary features!\n",
    "2. Additional feature engineering - we haven't considered any possible interaction effects. Check out [this resource for more information on interaction effects](https://stattrek.com/multiple-regression/interaction).\n",
    "3. Also on feature engineering, we could try modifying our input features. Instead of having one variable per job, maybe we could group them by approximate income level. We could try using weekdays and weekends instead of just `day_of_week`. Separating months into seasons could also help. You don't know until you try!\n",
    "4. Maybe we should standardise/normalise our data in other ways. For instance, we didn't handle the special case of `pdays==999`. We could probably handle this in several different ways - for example, replacing those values with the highest non-999 value of `pdays`, omitting it entirely (i.e. making it a `NaN`), create a separate dummy variable to indicate it, etc.\n",
    "5. There are other ways of tackling the unbalanced classification problem. Maybe if we tried those, our model would perform better?\n",
    "\n",
    "As with Quest 2, attempting further improvements are left as an exercise to the reader ;)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f943bec",
   "metadata": {},
   "source": [
    "# solution to get list of all variable combinations from highest correlation to lowest\n",
    "all_vars = []\n",
    "var_names = df.columns\n",
    "for row_num, row in enumerate(df.corr().itertuples(index=False)):\n",
    "    for col_num, value in enumerate(row):\n",
    "        if col_num == row_num: continue\n",
    "        all_vars.append((var_names[col_num], var_names[row_num], value))\n",
    "sorted(all_vars, key=lambda x: abs(x[2]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba857a",
   "metadata": {},
   "source": [
    "# Prepare your Submission\n",
    "Congratulations, you made it to the end of this lesson! Be sure to complete the few remaining quiz questions, and prepare your submission:\n",
    "1. Replace the value of the NAME variable with your StackUp name.\n",
    "2. Edit the last cell in the notebook so that it reflects the requested data, rather than the placeholder values below. Make sure you read what it is asking for, and replicate all the features of it - this includes axis labels, which charts go where, etc. \n",
    "3. Restart your notebook, then run it from start to end.\n",
    "4. Take a screenshot of the output from the last cell in this notebook. Make sure your screenshot shows your taskbar/dock (i.e. take a screenshot of the whole screen, not just the window).\n",
    "\n",
    "e.g. This section of code currently produces placeholder values:  \n",
    "\n",
    "```print(f\"{'intercept':<20}: {np.random.random():.4f}\")```\n",
    "\n",
    "You must replace `np.random.random()` so that it reflects the intercept of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace this with your stackup name\n",
    "NAME = \"yourstackupname\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill out this cell appropriately, then take a screenshot of the result\n",
    "print(f\"{'Name':<20}: {NAME}\")\n",
    "# describe model\n",
    "print(f\"{'intercept':<20}: {np.random.random():.4f}\")\n",
    "print(\"5 coefficients with largest absolute value:\")\n",
    "a = sorted([np.random.random()*2-1 for _ in range(5)], key=lambda x: abs(x), reverse=True)\n",
    "for i, val in enumerate(a):\n",
    "    print(f\"{df.columns[np.random.randint(0,len(df.columns))]:<20}: {val:.4f}\")\n",
    "print(\"---\"*8)\n",
    "print(f\"{'Score on test set':<20}: {np.random.random():.4f}\")\n",
    "print(f\"{'Plots for test set':<20}:\")\n",
    "# draw plots for test set\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "# ax1 - confusion matrix\n",
    "sns.heatmap(np.random.randint(100,1000,(2,2)), ax=ax1)\n",
    "\n",
    "# ax2 - draw RocCurveDisplay\n",
    "ax2.axline([0, 0], [1, 1])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b11c60c7f015efd0558dbbdc481384a1591da661b11b889df0e740e076d05c39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
